{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2026-01-07T16:11:36.255588Z\",\"iopub.execute_input\":\"2026-01-07T16:11:36.255907Z\",\"iopub.status.idle\":\"2026-01-07T16:11:39.538086Z\",\"shell.execute_reply.started\":\"2026-01-07T16:11:36.255880Z\",\"shell.execute_reply\":\"2026-01-07T16:11:39.537192Z\"}}\n# Install the grad-cam library\n!pip install grad-cam\n\n# %% [markdown]\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-07T17:36:07.777224Z\",\"iopub.execute_input\":\"2026-01-07T17:36:07.777950Z\",\"iopub.status.idle\":\"2026-01-07T17:36:07.786185Z\",\"shell.execute_reply.started\":\"2026-01-07T17:36:07.777908Z\",\"shell.execute_reply\":\"2026-01-07T17:36:07.785587Z\"}}\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset, Dataset\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision import models\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom PIL import Image\nimport seaborn as sns\nimport os\nimport copy\nfrom torch.autograd import Variable\nfrom collections import Counter\nimport random\n\n# Set device to GPU if available, otherwise CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\ndevice\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-07T17:36:08.308435Z\",\"iopub.execute_input\":\"2026-01-07T17:36:08.308911Z\",\"iopub.status.idle\":\"2026-01-07T17:36:08.314256Z\",\"shell.execute_reply.started\":\"2026-01-07T17:36:08.308870Z\",\"shell.execute_reply\":\"2026-01-07T17:36:08.313692Z\"}}\n\n# Define classes to use and create class mappings\nselected_classes = [3, 5, 7]\nclass_map = {3: 0, 5: 1, 7: 2}\nnum_classes = len(selected_classes)\nclass_names = ['cat', 'dog', 'horse']\n\n# Define training data augmentation and normalization\ntrain_transform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Define validation/test data normalization (no augmentation)\nval_transform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-07T17:36:08.586028Z\",\"iopub.execute_input\":\"2026-01-07T17:36:08.586238Z\",\"iopub.status.idle\":\"2026-01-07T17:37:27.830063Z\",\"shell.execute_reply.started\":\"2026-01-07T17:36:08.586218Z\",\"shell.execute_reply\":\"2026-01-07T17:37:27.829296Z\"}}\n\n# Load full CIFAR-10 dataset\nfull_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\nfull_testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=val_transform)\n\n# Get labels from datasets to check class distribution\ntrain_labels = [label for _, label in full_trainset]\ntest_labels = [label for _, label in full_testset]\n\nprint(\"Train class counts:\", Counter(train_labels))\nprint(\"Test class counts:\", Counter(test_labels))\n\n# Display a sample image from the dataset\nsample_img, sample_label = full_trainset[0]\nimg_display = sample_img.permute(1, 2, 0).numpy()\nimg_display = (img_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)\n\nplt.imshow(img_display)\nplt.title(f\"Sample: Class {full_trainset.classes[sample_label]}\")\nplt.axis('off')\nplt.show()\n\n\n# %% [markdown]\n# **All CIFAR-10 classes balanced** - 5,000 train and 1,000 test samples per class\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-07T17:37:27.831670Z\",\"iopub.execute_input\":\"2026-01-07T17:37:27.832049Z\",\"iopub.status.idle\":\"2026-01-07T17:38:58.768725Z\",\"shell.execute_reply.started\":\"2026-01-07T17:37:27.832019Z\",\"shell.execute_reply\":\"2026-01-07T17:38:58.767701Z\"}}\n\n# Create custom dataset class to filter only selected classes\nclass FilteredCIFAR10(Dataset):\n    def __init__(self, dataset, selected_classes, class_map):\n        self.dataset = dataset\n        self.indices = []\n\n        for i in range(len(dataset)):\n            _, label = dataset[i]\n            if label in selected_classes:\n                self.indices.append(i)\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        img, label = self.dataset[self.indices[idx]]\n        return img, class_map[label]\n\n\n# Apply filtering to train and test datasets\ntrainset_filtered = FilteredCIFAR10(full_trainset, selected_classes, class_map)\ntestset_filtered = FilteredCIFAR10(full_testset, selected_classes, class_map)\n\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-07T17:38:58.770043Z\",\"iopub.execute_input\":\"2026-01-07T17:38:58.770414Z\",\"iopub.status.idle\":\"2026-01-07T17:39:41.268096Z\",\"shell.execute_reply.started\":\"2026-01-07T17:38:58.770362Z\",\"shell.execute_reply\":\"2026-01-07T17:39:41.267274Z\"}}\n\n# Set maximum number of samples and split ratios\nmax_total = 5000\ntrain_ratio = 0.8\nper_class_total = max_total // num_classes\nper_class_train = int(per_class_total * train_ratio)\nper_class_test = per_class_total - per_class_train\n\n# Function to get indices for each class\ndef get_class_indices(dataset):\n    class_indices = {c: [] for c in range(num_classes)}\n    for i in range(len(dataset)):\n        _, label = dataset[i]\n        class_indices[label].append(i)\n    return class_indices\n\n# Get class indices for balanced sampling\ntrain_class_indices = get_class_indices(trainset_filtered)\ntest_class_indices = get_class_indices(testset_filtered)\n\n# Sample equal number of examples per class\ntrain_indices = []\ntest_indices = []\n\nfor c in range(num_classes):\n    train_indices += random.sample(train_class_indices[c], per_class_train)\n    test_indices += random.sample(test_class_indices[c], per_class_test)\n\n# Create subsets with balanced classes\ntrainset_filtered = Subset(trainset_filtered, train_indices)\ntestset_filtered = Subset(testset_filtered, test_indices)\n\nprint(f\"Train: {len(trainset_filtered)}, Test: {len(testset_filtered)}, Total: {len(trainset_filtered)+len(testset_filtered)}\")\n\n\n# Verify class distribution in filtered datasets\nfiltered_train_labels = [trainset_filtered[i][1] for i in range(len(trainset_filtered))]\nfiltered_test_labels = [testset_filtered[i][1] for i in range(len(testset_filtered))]\n\nprint(\"\\nFiltered train class counts:\", Counter(filtered_train_labels))\nprint(\"Filtered test class counts:\", Counter(filtered_test_labels))\nprint(f\"Total filtered train samples: {len(trainset_filtered)}\")\nprint(f\"Total filtered test samples: {len(testset_filtered)}\")\n\n# Display one sample image from each class\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\nfor idx, class_label in enumerate([0, 1, 2]):\n    for i in range(len(trainset_filtered)):\n        img, label = trainset_filtered[i]\n        if label == class_label:\n            img_display = img.permute(1, 2, 0).numpy()\n            img_display = (img_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)\n            axes[idx].imshow(img_display)\n            axes[idx].set_title(f\"Class {class_label}: {class_names[class_label]}\")\n            axes[idx].axis('off')\n            break\n\nplt.tight_layout()\nplt.show()\n\n\n# %% [markdown]\n# **Balanced 3-class subset created** - Total 4,998 samples (≤5,000 requirement met)\n# **Equal class distribution** - Each class has ~1,332 train and 334 test samples\n\n# %% [code]\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-07T17:39:41.269877Z\",\"iopub.execute_input\":\"2026-01-07T17:39:41.270188Z\",\"iopub.status.idle\":\"2026-01-07T17:39:47.732202Z\",\"shell.execute_reply.started\":\"2026-01-07T17:39:41.270161Z\",\"shell.execute_reply\":\"2026-01-07T17:39:47.731285Z\"}}\n\n# Split training data into train and validation sets using stratified sampling\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nlabels = [trainset_filtered[i][1] for i in range(len(trainset_filtered))]\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\ntrain_idx, val_idx = next(sss.split(np.zeros(len(labels)), labels))\n\ntrain_dataset = Subset(trainset_filtered, train_idx)\nval_dataset = Subset(trainset_filtered, val_idx)\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-07T17:39:47.733354Z\",\"iopub.execute_input\":\"2026-01-07T17:39:47.733644Z\",\"iopub.status.idle\":\"2026-01-07T17:39:47.741490Z\",\"shell.execute_reply.started\":\"2026-01-07T17:39:47.733615Z\",\"shell.execute_reply\":\"2026-01-07T17:39:47.740609Z\"}}\n# Create data loaders for batching\nbatch_size = 32\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(testset_filtered, batch_size=batch_size, shuffle=False)\n\nprint(f\"\\nTraining samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}, Test samples: {len(testset_filtered)}\")\n\n\n# %% [markdown]\n# \n# **Stratified 80/20 split** - 3,196 train, 800 validation, 1,002 test samples\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-07T17:47:47.831875Z\",\"iopub.execute_input\":\"2026-01-07T17:47:47.832477Z\",\"iopub.status.idle\":\"2026-01-07T17:47:48.040713Z\",\"shell.execute_reply.started\":\"2026-01-07T17:47:47.832443Z\",\"shell.execute_reply\":\"2026-01-07T17:47:48.039948Z\"}}\n\n# Load pre-trained ResNet18 model\nmodel = models.resnet18(pretrained=True)\n\n# Freeze all layers except the final classifier\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace final layer for our 3 classes\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\nmodel = model.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-07T17:54:56.416422Z\",\"iopub.execute_input\":\"2026-01-07T17:54:56.417255Z\",\"iopub.status.idle\":\"2026-01-07T17:57:27.456953Z\",\"shell.execute_reply.started\":\"2026-01-07T17:54:56.417220Z\",\"shell.execute_reply\":\"2026-01-07T17:57:27.456262Z\"}}\n\n# Define training function with validation\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=15):\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        \n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * inputs.size(0)\n            _, preds = torch.max(outputs, 1)\n            train_correct += torch.sum(preds == labels.data)\n        \n        train_loss /= len(train_loader.dataset)\n        train_acc = train_correct.double() / len(train_loader.dataset)\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        \n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                \n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item() * inputs.size(0)\n                _, preds = torch.max(outputs, 1)\n                val_correct += torch.sum(preds == labels.data)\n        \n        val_loss /= len(val_loader.dataset)\n        val_acc = val_correct.double() / len(val_loader.dataset)\n        \n        print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n        \n        # Save best model based on validation accuracy\n        if val_acc > best_acc:\n            best_acc = val_acc\n            best_model_wts = copy.deepcopy(model.state_dict())\n    \n    model.load_state_dict(best_model_wts)\n    return model\n\n# Train the model\nmodel = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=15)\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-07T17:57:27.458423Z\",\"iopub.execute_input\":\"2026-01-07T17:57:27.458704Z\",\"iopub.status.idle\":\"2026-01-07T17:57:27.550716Z\",\"shell.execute_reply.started\":\"2026-01-07T17:57:27.458682Z\",\"shell.execute_reply\":\"2026-01-07T17:57:27.550096Z\"}}\n\n# Save the trained model\nimport os\nsave_path = '/kaggle/working/model.pth' if 'kaggle' in os.getcwd() else 'model.pth'\ntorch.save(model.state_dict(), save_path)\nprint(f\"Best model saved to {save_path}\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-07T17:57:27.551562Z\",\"iopub.execute_input\":\"2026-01-07T17:57:27.551872Z\",\"iopub.status.idle\":\"2026-01-07T17:57:30.411849Z\",\"shell.execute_reply.started\":\"2026-01-07T17:57:27.551839Z\",\"shell.execute_reply\":\"2026-01-07T17:57:30.411203Z\"}}\n\n# Define evaluation function to test model performance\ndef evaluate_model(model, test_loader, class_names):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    correct_examples = []\n    incorrect_examples = []\n    max_correct = 20\n    max_incorrect = 20\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            \n            # Collect examples for visualization\n            for i in range(inputs.size(0)):\n                if preds[i] == labels[i] and len(correct_examples) < max_correct:\n                    correct_examples.append((inputs[i].cpu(), preds[i].cpu(), labels[i].cpu()))\n                elif preds[i] != labels[i] and len(incorrect_examples) < max_incorrect:\n                    incorrect_examples.append((inputs[i].cpu(), preds[i].cpu(), labels[i].cpu()))\n    \n    # Calculate and display accuracy\n    accuracy = accuracy_score(all_labels, all_preds)\n    print(f'\\nTest Accuracy: {accuracy:.4f}')\n    \n    # Display confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(6,6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()\n    \n    # Display correct predictions\n    print(f'\\n{min(5, len(correct_examples))} Correct Predictions:')\n    fig, axes = plt.subplots(1, min(5, len(correct_examples)), figsize=(15, 3))\n    if len(correct_examples) == 1:\n        axes = [axes]\n    \n    for i, (img, pred, label) in enumerate(correct_examples[:5]):\n        img = img.permute(1, 2, 0).numpy()\n        img = (img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0,1)\n        axes[i].imshow(img)\n        axes[i].set_title(f'Pred: {class_names[pred]}\\nTrue: {class_names[label]}')\n        axes[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Display incorrect predictions\n    print(f'{min(5, len(incorrect_examples))} Incorrect Predictions:')\n    fig, axes = plt.subplots(1, min(5, len(incorrect_examples)), figsize=(15, 3))\n    if len(incorrect_examples) == 1:\n        axes = [axes]\n    \n    for i, (img, pred, label) in enumerate(incorrect_examples[:5]):\n        img = img.permute(1, 2, 0).numpy()\n        img = (img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0,1)\n        axes[i].imshow(img)\n        axes[i].set_title(f'Pred: {class_names[pred]}\\nTrue: {class_names[label]}')\n        axes[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print detailed classification report\n    from sklearn.metrics import classification_report\n    print(\"\\nPer-Class Performance:\")\n    print(classification_report(all_labels, all_preds, target_names=class_names))\n    \n    return correct_examples, incorrect_examples\n\n# Evaluate the model on test set\ncorrect_examples, incorrect_examples = evaluate_model(model, test_loader, class_names)\n\n\n# %% [markdown]\n# **83% Test Accuracy achieved**\n# **Horse class performs best** (90% F1-score)\n# **Cat class slightly weaker** (78% F1-score) - possible confusion with dog features\n# **Balanced performance** across all classes (macro avg = 0.83)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2026-01-07T17:57:30.413493Z\",\"iopub.execute_input\":\"2026-01-07T17:57:30.413777Z\",\"iopub.status.idle\":\"2026-01-07T17:57:32.871291Z\",\"shell.execute_reply.started\":\"2026-01-07T17:57:30.413755Z\",\"shell.execute_reply\":\"2026-01-07T17:57:32.870730Z\"}}\n\n# Generate Grad-CAM visualizations to understand model decisions\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\nmodel.eval()\nmodel.layer4.requires_grad_(True)\n\n# Function to denormalize images for visualization\ndef prepare_image(img_tensor):\n    mean = np.array([0.485, 0.456, 0.406]).reshape(3,1,1)\n    std = np.array([0.229, 0.224, 0.225]).reshape(3,1,1)\n    img = img_tensor.cpu().numpy()\n    img = img * std + mean\n    img = np.transpose(img, (1,2,0))\n    return np.clip(img, 0, 1)\n\n# Target the last layer of ResNet18 for Grad-CAM\ntarget_layers = [model.layer4[-1]]\n\nwith GradCAM(model=model, target_layers=target_layers) as cam:\n    # Visualize correct predictions\n    for i, (img_tensor, pred, true) in enumerate(correct_examples[:5]):\n        cam_map = cam(\n            input_tensor=img_tensor.unsqueeze(0).to(device),\n            targets=[ClassifierOutputTarget(pred)]\n        )[0]\n        \n        img_np = prepare_image(img_tensor)\n        overlay = show_cam_on_image(img_np, cam_map, use_rgb=True)\n        \n        plt.figure(figsize=(10,3))\n        \n        plt.subplot(1,3,1)\n        plt.imshow(img_np)\n        plt.title(f\"Original\\nTrue: {class_names[true]}\")\n        plt.axis(\"off\")\n        \n        plt.subplot(1,3,2)\n        plt.imshow(cam_map, cmap=\"jet\")\n        plt.title(\"Grad-CAM\")\n        plt.axis(\"off\")\n        \n        plt.subplot(1,3,3)\n        plt.imshow(overlay)\n        plt.title(f\"Overlay\\nPred: {class_names[pred]}\")\n        plt.axis(\"off\")\n        \n        plt.suptitle(\"CORRECT Prediction\", fontsize=13)\n        plt.tight_layout()\n        plt.show()\n        \n        focus_intensity = np.mean(cam_map)\n        print(f\"Image {i+1} - Interpretation: Strong focus (intensity: {focus_intensity:.2f}) on key features like the {class_names[true].lower()}'s shape, leading to accurate prediction.\")\n    \n    # Visualize incorrect predictions\n    for i, (img_tensor, pred, true) in enumerate(incorrect_examples[:5]):\n        cam_map = cam(\n            input_tensor=img_tensor.unsqueeze(0).to(device),\n            targets=[ClassifierOutputTarget(pred)]\n        )[0]\n        \n        img_np = prepare_image(img_tensor)\n        overlay = show_cam_on_image(img_np, cam_map, use_rgb=True)\n        \n        true_cam_map = cam(\n            input_tensor=img_tensor.unsqueeze(0).to(device),\n            targets=[ClassifierOutputTarget(true)]\n        )[0]\n        \n        true_overlay = show_cam_on_image(img_np, true_cam_map, use_rgb=True)\n        \n        plt.figure(figsize=(13,3))\n        \n        plt.subplot(1,4,1)\n        plt.imshow(img_np)\n        plt.title(f\"Original\\nTrue: {class_names[true]}\")\n        plt.axis(\"off\")\n        \n        plt.subplot(1,4,2)\n        plt.imshow(cam_map, cmap=\"jet\")\n        plt.title(\"Pred Grad-CAM\")\n        plt.axis(\"off\")\n        \n        plt.subplot(1,4,3)\n        plt.imshow(overlay)\n        plt.title(f\"Pred Overlay\\nPred: {class_names[pred]}\")\n        plt.axis(\"off\")\n        \n        plt.subplot(1,4,4)\n        plt.imshow(true_overlay)\n        plt.title(f\"True Overlay\\nTrue: {class_names[true]}\")\n        plt.axis(\"off\")\n        \n        plt.suptitle(\"INCORRECT Prediction\", fontsize=13)\n        plt.tight_layout()\n        plt.show()\n        \n        focus_intensity = np.mean(cam_map)\n        print(f\"Image {i+1} - Interpretation: Misplaced focus (intensity: {focus_intensity:.2f}) possibly on background or similar features, causing confusion with {class_names[pred].lower()}.\")\n\n# %% [markdown]\n# Here's a concise markdown version:\n# \n# ---\n# \n# ## **Grad-CAM Explainability Analysis**\n# \n# ### **✅ Correct Predictions - What Works**\n# - **Strong focused attention** (intensity ~0.3-0.4) on key features: animal shape, head, body outline\n# - **Foreground isolation** - attention stays on subject, ignores background\n# - **Class-specific traits captured** - elongated bodies (horse), compact forms (cat/dog)\n# \n# ---\n# \n# ### **❌ Incorrect Predictions - Where Model Fails**\n# \n# **1. Localized Focus Problem**\n# - Attention on small regions (body mass only), misses limbs/tails/face\n# - Fails to capture holistic shape → confuses similar classes (cat ↔ dog)\n# \n# **2. Background Interference**\n# - Attention spills to irrelevant areas (environment, objects)\n# - Learns spurious correlations, not true animal features\n# \n# **3. Texture Over Structure**\n# - Prioritizes superficial cues (fur patterns, color)\n# - Ignores structural differences (ear shape, posture)\n# - Common cat/dog confusion due to similar textures\n# \n# **4. Poor Image Quality**\n# - Blurry/low-res images → diffuse, weak attention\n# - Can't lock onto meaningful features → higher error rate\n# \n# ---\n\n# %% [code]\n","metadata":{"_uuid":"2fb3a390-d7f9-435b-9d04-6ab571aad3d9","_cell_guid":"bd425d61-09ed-439d-ab7f-3dfcccb61d7a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}